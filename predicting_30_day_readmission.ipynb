{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7qrRtY1Sx_N"
      },
      "source": [
        "**Import Data and Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hrS_QhCRFVN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow.feather as feather\n",
        "\n",
        "df = feather.read_feather('/content/diabetes.feather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpKZW0WPenSR"
      },
      "source": [
        "1) Prepare the x and y objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-SeuiKCGWso"
      },
      "outputs": [],
      "source": [
        "df = pd.get_dummies(df, columns=['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id',\n",
        "                                 'admission_source_id', 'diag_1', 'diag_2', 'diag_3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG4BA71DgNui"
      },
      "outputs": [],
      "source": [
        "# drop the 'readmitted' to create x  and create y\n",
        "x = df.drop(columns=['readmitted'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV2h3KQBgkoO"
      },
      "outputs": [],
      "source": [
        "y = df[\"readmitted\"]\n",
        "y = (y == \">30\").astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQNimXhEkZIG"
      },
      "source": [
        "2) Split the data into two sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghedge3Tj6Xx"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and test sets with 80/20 split and stratify on y\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size = 0.2,random_state = 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkVhkFN6j56m"
      },
      "source": [
        "3) Build initial models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsMw2q-Fky4D"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Instantiate the models\n",
        "mod_rf = RandomForestClassifier(n_estimators=500, random_state=1)\n",
        "mod_boost = GradientBoostingClassifier(n_estimators=500, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZIrKVxiP0e_",
        "outputId": "43f8e248-9efd-4905-a80e-98679c5ce2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF CV F1-score: 0.1944029337430245\n",
            "GB CV F1-score: 0.1416202132362271\n"
          ]
        }
      ],
      "source": [
        "# Perform 10-fold cross-validation and calculate F1-score for each model\n",
        "f1_rf = cross_val_score(mod_rf, x_train, y_train, cv=10, scoring='f1')\n",
        "f1_boost = cross_val_score(mod_boost, x_train, y_train, cv=10, scoring='f1')\n",
        "\n",
        "cv_f1_rf = f1_rf.mean()\n",
        "cv_f1_boost = f1_boost.mean()\n",
        "\n",
        "print('RF CV F1-score:', cv_f1_rf)\n",
        "print('GB CV F1-score:', cv_f1_boost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1qgGK6bskg8"
      },
      "source": [
        "4)  Build models with tuned hyperpamrameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SmnhXUOsoON",
        "outputId": "b7c6022f-3582-4386-b1da-e0035bb3790e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1-score for Random Forest: 0.16013333046961506\n",
            "Best F1-score for Gradient Boosting: 0.24029735468412716\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the parameter grids for Random Forest and Gradient Boosting\n",
        "params_rf = {\n",
        "    'n_estimators': [100, 300, 500, 800, 1000],\n",
        "    'max_depth': [5, 10, 15, 20, 25, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "params_boost = {\n",
        "    'n_estimators': [100, 300, 500, 800, 1000],\n",
        "    'learning_rate': [0.001, 0.01, 0.1, 0.5, 1],\n",
        "    'max_depth': [3, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Perform random search on Random Forest and Gradient Boosting\n",
        "rs_rf = RandomizedSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    params_rf,\n",
        "    cv=10,\n",
        "    n_iter=6,\n",
        "    scoring='f1',\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "rs_boost = RandomizedSearchCV(\n",
        "    GradientBoostingClassifier(),\n",
        "    params_boost,\n",
        "    cv=10,\n",
        "    n_iter=6,\n",
        "    scoring='f1',\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# Fit the models and print the best F1-score\n",
        "rs_rf.fit(x_train, y_train)\n",
        "rs_boost.fit(x_train, y_train)\n",
        "\n",
        "print('Best F1-score for Random Forest:', rs_rf.best_score_)\n",
        "print('Best F1-score for Gradient Boosting:', rs_boost.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6m3zmDvvpmV"
      },
      "source": [
        "5) Build models that undersample negative cases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_fold1, x_fold2, y_fold1, y_fold2 = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming y_fold1 is the target variable containing 0s and 1s\n",
        "pos_indx = np.where(y_fold1 == 1)[0] # get the indices where y_fold1 is 1\n",
        "neg_indx = np.where(y_fold1 == 0)[0] # get the indices where y_fold1 is \n",
        "\n",
        "m = int(np.floor(len(pos_indx) * 0.5))\n",
        "np.random.seed(1)\n",
        "\n",
        "sample_indx = np.random.choice(neg_indx, size=m, replace=False)\n",
        "\n",
        "x_subsampled = pd.concat([x_fold1.iloc[pos_indx], x_fold1.iloc[sample_indx]])\n",
        "y_subsampled = pd.concat([y_fold1.iloc[pos_indx], y_fold1.iloc[sample_indx]])"
      ],
      "metadata": {
        "id": "r49mKKUU4u2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Train a Random Forest classifier with 500 decision trees\n",
        "mod_rf_2 = RandomForestClassifier(n_estimators=500, random_state=1).fit(x_subsampled, y_subsampled)\n",
        "\n",
        "# Train a Gradient Boosting classifier with 500 decision trees\n",
        "mod_boost_2 = GradientBoostingClassifier(n_estimators=500, random_state=1).fit(x_subsampled, y_subsampled)"
      ],
      "metadata": {
        "id": "1-tTeJdX9fC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def get_f1(y_true, y_pred):\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    if not f1:\n",
        "        return 0\n",
        "    else:\n",
        "        return f1\n",
        "\n",
        "pred_mod_rf_2 = mod_rf_2.predict(x_fold2)\n",
        "pred_mod_boost_2 = mod_boost_2.predict(x_fold2)"
      ],
      "metadata": {
        "id": "Iblb-6Vu9wNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1 scores for mod_rf_2 and mod_boost_2\n",
        "f1_mod_rf_2 = get_f1(y_fold2, pred_mod_rf_2)\n",
        "f1_mod_boost_2 = get_f1(y_fold2, pred_mod_boost_2)\n",
        "\n",
        "# Print the F1 scores\n",
        "print(\"Random Forest F1 score:\", f1_mod_rf_2)\n",
        "print(\"Gradient Boosting F1 score:\", f1_mod_boost_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M21GW1ze-dAR",
        "outputId": "ca0dfacd-d96c-4d8b-8983-b1fe2f878051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest F1 score: 0.47626495659654333\n",
            "Gradient Boosting F1 score: 0.47964628754383287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Make a dataframe of model specifications (We attempted steps 6 to 9 but stopped at 7.4 since they are optional)"
      ],
      "metadata": {
        "id": "nJqBQEaU-wRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_n_estimators = pd.DataFrame({\"n_estimators\": np.arange(200, 1001, 200)})\n",
        "df_min_samples_leaf = pd.DataFrame({\"min_samples_leaf\": np.arange(0.01, 0.1, 0.02)})\n",
        "df_max_depth = pd.DataFrame({\"max_depth\": np.arange(1, 6, 1)})\n",
        "df_subsample = pd.DataFrame({\"subsample\": [0.5, 1, 2]})\n",
        "df_max_depth[\"id\"] = 1\n",
        "df_n_estimators[\"id\"] = 1\n",
        "df_min_samples_leaf[\"id\"] = 1\n",
        "df_subsample[\"id\"] = 1\n",
        "df_rf = pd.merge(df_n_estimators, df_min_samples_leaf, on = \"id\")\n",
        "df_rf = pd.merge(df_rf, df_subsample, on = \"id\")\n",
        "df_rf[\"estimator\"] = \"Random Forest\"\n",
        "df_rf[\"max_depth\"] = None\n",
        "df_rf = df_rf.drop(\"id\", axis = 1)\n",
        "df_boost = pd.merge(df_n_estimators, df_max_depth, on = \"id\")\n",
        "df_boost = pd.merge(df_boost, df_subsample, on = \"id\")\n",
        "df_boost[\"estimator\"] = \"Boosting\"\n",
        "df_boost[\"min_samples_leaf\"] = None\n",
        "df_boost = df_boost.drop(\"id\", axis = 1)\n",
        "df_models = pd.concat([df_rf, df_boost], ignore_index = True)"
      ],
      "metadata": {
        "id": "scNBbC0z-4TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Select the best model using CV"
      ],
      "metadata": {
        "id": "UYTtVCLJEMrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_models = df_models.shape[0]\n",
        "\n",
        "n_models = len(df_models)\n",
        "cv_rslt = np.zeros((n_models, 10))"
      ],
      "metadata": {
        "id": "wDfkkWslEeOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ceil(len(y_train) / 10)\n",
        "fold_vec = np.concatenate([np.arange(10)] * int(n))\n",
        "fold_vec = fold_vec[0:len(y)]\n",
        "np.random.seed(1)\n",
        "fold_vec = np.random.permutation(fold_vec)"
      ],
      "metadata": {
        "id": "Onbatb1uFZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "K2DRm5ZIGZ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7.4\n",
        "for i in range(10):\n",
        "  indx_1fold = ...\n",
        "  indx_9fold = ...\n",
        "  x_1fold = ...\n",
        "  y_1fold = ...\n",
        "  x_9fold = ...\n",
        "  y_9fold = ...\n",
        "\n",
        "\n",
        "  for j in range(n_models):\n",
        "    # Get the jth model specifics\n",
        "    estimator = ...\n",
        "    n_estimators = ...\n",
        "    min_samples_leaf = ...\n",
        "    subsample = ...\n",
        "    max_depth = ...\n",
        "    # Instantiate the model\n",
        "    if estimator == \"Random Forest\":\n",
        "      mod = ...\n",
        "    if estimator == \"Boosting\":\n",
        "      mod = ...\n",
        "    # Subsample the negative cases, using the object \"subsample\"\n",
        "    pos_indx = ...\n",
        "    neg_indx = ...\n",
        "    m = ...\n",
        "    np.random.seed(i)\n",
        "    sample_indx = ...\n",
        "    x_subsampled = ...\n",
        "    y_subsampled = ...\n",
        "    mod.fit(..., ...)\n",
        "    pred_mod = ...\n",
        "    cv_rslt... = ..."
      ],
      "metadata": {
        "id": "q_L4dYb2LRZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}